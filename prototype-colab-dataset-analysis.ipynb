{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75142d1",
   "metadata": {},
   "source": [
    "## üîß Quick Setup: Change to Python 3.11 Environment\n",
    "\n",
    "### Option 1: Use the Setup Script (Easiest!)\n",
    "\n",
    "**In PowerShell terminal (in VS Code or standalone):**\n",
    "\n",
    "```powershell\n",
    "# Navigate to this directory\n",
    "cd \"d:\\project zip flies\\scala project\\scala project\"\n",
    "\n",
    "# Run the setup script\n",
    ".\\setup-spark-env.bat\n",
    "```\n",
    "\n",
    "This will automatically:\n",
    "- ‚úÖ Create Python 3.11 environment\n",
    "- ‚úÖ Install all required packages\n",
    "- ‚úÖ Offer to start Jupyter for you\n",
    "\n",
    "---\n",
    "\n",
    "### Option 2: Manual Commands\n",
    "\n",
    "**In PowerShell/Terminal:**\n",
    "\n",
    "```powershell\n",
    "# Create environment\n",
    "conda create -n spark-env python=3.11 -y\n",
    "\n",
    "# Activate it\n",
    "conda activate spark-env\n",
    "\n",
    "# Install packages\n",
    "pip install pyspark==3.5.0 findspark pandas numpy matplotlib seaborn plotly textblob jupyter\n",
    "\n",
    "# Start Jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Option 3: Check Current Environment\n",
    "\n",
    "**Run this in a cell to see your current Python version:**\n",
    "\n",
    "```python\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Executable: {sys.executable}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**After setup, reopen this notebook in the new Jupyter session and Spark jobs will work!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65579840",
   "metadata": {},
   "source": [
    "# üìä Dataset Analysis Prototype - Spark Web UI Edition\n",
    "\n",
    "## üöÄ Quick Start (Windows Local Jupyter)\n",
    "\n",
    "### ‚ö†Ô∏è CRITICAL: Python Version for Spark Jobs\n",
    "\n",
    "**Tested & Working:** Python 3.10, Python 3.11  \n",
    "**Issues Reported:** Python 3.12 (worker crashes), Python 3.13 (not compatible)\n",
    "\n",
    "**Current Status:**\n",
    "- ‚úÖ Python 3.10/3.11 ‚Üí Spark jobs work perfectly\n",
    "- ‚ö†Ô∏è Python 3.12 ‚Üí May have worker crash issues  \n",
    "- ‚ùå Python 3.13 ‚Üí Not supported by PySpark 3.5.0\n",
    "\n",
    "### üîß Setup Python 3.11 (Recommended for Spark)\n",
    "\n",
    "**Method 1: Anaconda/Miniconda (Easiest)**\n",
    "```powershell\n",
    "# Create environment with Python 3.11\n",
    "conda create -n spark-env python=3.11 -y\n",
    "conda activate spark-env\n",
    "\n",
    "# Install packages\n",
    "pip install pyspark==3.5.0 findspark pandas numpy matplotlib seaborn plotly textblob jupyter\n",
    "\n",
    "# Start Jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "**Method 2: Manual Installation**\n",
    "1. Download Python 3.11: https://www.python.org/downloads/release/python-3119/\n",
    "2. Install to: `C:\\Python311`\n",
    "3. Create virtual environment:\n",
    "```powershell\n",
    "C:\\Python311\\python.exe -m venv spark-venv\n",
    ".\\spark-venv\\Scripts\\activate\n",
    "pip install pyspark==3.5.0 findspark pandas numpy matplotlib seaborn plotly textblob jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "**Alternative: Try older PySpark with Python 3.12**\n",
    "```powershell\n",
    "pip uninstall pyspark -y\n",
    "pip install pyspark==3.4.1\n",
    "```\n",
    "\n",
    "### üìã Workflow\n",
    "\n",
    "**Setup:**\n",
    "1. ‚úÖ Run Cell 3 - Install packages\n",
    "2. ‚úÖ Run Cell 6 - Windows Spark setup\n",
    "3. ‚úÖ Run Cell 8 - Verify Java (optional)\n",
    "4. ‚úÖ Run Cell 18 - Readiness check\n",
    "\n",
    "**Analysis:**\n",
    "5. ‚úÖ Run Cell 11 or 12 - Upload CSV\n",
    "6. ‚úÖ Run Cell 19 - Generate Spark jobs (Python 3.10/3.11 only)\n",
    "   - Or skip and use pandas analysis\n",
    "7. ‚úÖ Run Cells 20-28 - Complete data analysis\n",
    "\n",
    "**Spark Web UI:** http://localhost:4040\n",
    "\n",
    "### üí° If Spark Jobs Fail\n",
    "\n",
    "**Don't worry!** You can still get complete analysis:\n",
    "- Skip Cell 19 (Spark jobs)\n",
    "- Run Cells 20-28 (pandas analysis)\n",
    "- Get all visualizations, statistics, and insights\n",
    "- Spark UI won't show jobs, but everything else works!\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Sample CSV Files Available\n",
    "- `BoatProduct.csv`\n",
    "- `sample-products.csv`\n",
    "- `flipkart-phones-fresh.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6a1f57",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1be1949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages installed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn plotly textblob wordcloud scikit-learn -q\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2b367",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Step 1.5: Setup Spark with Ngrok (GOOGLE COLAB ONLY)\n",
    "\n",
    "**üö´ SKIP THIS CELL if using local Jupyter on Windows!**\n",
    "\n",
    "**For Google Colab users only:**\n",
    "- ‚ö° Installs Apache Spark  \n",
    "- üåç Public Spark Web UI access via ngrok\n",
    "- üìä Monitor jobs from anywhere\n",
    "\n",
    "**For local Windows Jupyter users:**\n",
    "- ‚úÖ Use Cell 5 instead: \"ü™ü Windows Local Jupyter - Setup Spark\"\n",
    "- ‚úÖ No ngrok needed - access at http://localhost:4040\n",
    "- ‚úÖ Skip this cell to avoid ModuleNotFoundError\n",
    "\n",
    "---\n",
    "\n",
    "**If you already ran Cell 5 (Windows Setup), skip this cell!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a823f49",
   "metadata": {},
   "source": [
    "## ü™ü Windows Local Jupyter - Setup Spark\n",
    "\n",
    "**For local Windows Jupyter notebook.** This version works without apt-get or Linux commands.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Java 8 or 11 installed\n",
    "- PySpark installed: `pip install pyspark findspark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef3357d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up Spark for Windows Jupyter...\n",
      "‚úÖ PySpark already installed\n",
      "‚úÖ Findspark initialized\n",
      "\n",
      "‚ö° Creating Spark session on port 4041...\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:180)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:197)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m------------------------------\u001b[39m",
      "\u001b[31mPy4JError\u001b[39mTraceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚ö° Creating Spark session on port \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark_ui_port\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Create Spark Session with reduced memory for better compatibility\u001b[39;00m\n\u001b[32m     57\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDataset Analysis - Jupyter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.ui.port\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mspark_ui_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.executor.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.ui.enabled\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.execution.arrow.pyspark.enabled\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfalse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Get the actual Web UI URL\u001b[39;00m\n\u001b[32m     68\u001b[39m spark_ui_url = spark.sparkContext.uiWebUrl\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\project zip flies\\scala project\\spark-python311-env\\Lib\\site-packages\\pyspark\\sql\\session.py:500\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    497\u001b[39m     sc = SparkContext.getOrCreate(sparkConf)\n\u001b[32m    498\u001b[39m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m     session = \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m    503\u001b[39m         \u001b[38;5;28mgetattr\u001b[39m(session._jvm, \u001b[33m\"\u001b[39m\u001b[33mSparkSession$\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mMODULE$\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     ).applyModifiableSettings(session._jsparkSession, \u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\project zip flies\\scala project\\spark-python311-env\\Lib\\site-packages\\pyspark\\sql\\session.py:589\u001b[39m, in \u001b[36mSparkSession.__init__\u001b[39m\u001b[34m(self, sparkContext, jsparkSession, options)\u001b[39m\n\u001b[32m    585\u001b[39m         \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._jvm, \u001b[33m\"\u001b[39m\u001b[33mSparkSession$\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mMODULE$\u001b[39m\u001b[33m\"\u001b[39m).applyModifiableSettings(\n\u001b[32m    586\u001b[39m             jsparkSession, options\n\u001b[32m    587\u001b[39m         )\n\u001b[32m    588\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m         jsparkSession = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    591\u001b[39m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._jvm, \u001b[33m\"\u001b[39m\u001b[33mSparkSession$\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mMODULE$\u001b[39m\u001b[33m\"\u001b[39m).applyModifiableSettings(\n\u001b[32m    592\u001b[39m         jsparkSession, options\n\u001b[32m    593\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\project zip flies\\scala project\\spark-python311-env\\Lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1581\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1582\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1583\u001b[39m     args_command +\\\n\u001b[32m   1584\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1586\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1587\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1591\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\project zip flies\\scala project\\spark-python311-env\\Lib\\site-packages\\py4j\\protocol.py:330\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    326\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    334\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    335\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    336\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name))\n",
      "\u001b[31mPy4JError\u001b[39m: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:180)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:197)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\n"
     ]
    }
   ],
   "source": [
    "# Windows-compatible Spark setup for Jupyter\n",
    "import os\n",
    "import sys\n",
    "import socket\n",
    "\n",
    "print(\"üîß Setting up Spark for Windows Jupyter...\")\n",
    "\n",
    "# Install PySpark if not already installed\n",
    "try:\n",
    "    import findspark\n",
    "    from pyspark.sql import SparkSession\n",
    "    print(\"‚úÖ PySpark already installed\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing PySpark...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyspark\", \"findspark\", \"-q\"])\n",
    "    import findspark\n",
    "    from pyspark.sql import SparkSession\n",
    "    print(\"‚úÖ PySpark installed\")\n",
    "\n",
    "# Initialize findspark (finds Spark installation)\n",
    "try:\n",
    "    findspark.init()\n",
    "    print(\"‚úÖ Findspark initialized\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Findspark init skipped (using system PySpark)\")\n",
    "\n",
    "# Import Spark functions\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Stop existing Spark session if any\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"üîÑ Stopped existing Spark session\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Function to find free port\n",
    "def find_free_port(start_port=4040):\n",
    "    \"\"\"Find a free port starting from start_port\"\"\"\n",
    "    port = start_port\n",
    "    while port < start_port + 100:\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            port += 1\n",
    "    return start_port\n",
    "\n",
    "# Find available port for Spark UI\n",
    "spark_ui_port = find_free_port(4040)\n",
    "\n",
    "print(f\"\\n‚ö° Creating Spark session on port {spark_ui_port}...\")\n",
    "\n",
    "# Create Spark Session with reduced memory for better compatibility\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Dataset Analysis - Jupyter\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.ui.port\", str(spark_ui_port)) \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.ui.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get the actual Web UI URL\n",
    "spark_ui_url = spark.sparkContext.uiWebUrl\n",
    "if not spark_ui_url:\n",
    "    spark_ui_url = f\"http://localhost:{spark_ui_port}\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ SPARK SESSION CREATED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Spark UI Port: {spark_ui_port}\")\n",
    "print(f\"   Local Web UI: {spark_ui_url}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display clickable link\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "html = f'''\n",
    "<div style=\"background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%); \n",
    "            padding: 25px; border-radius: 12px; text-align: center; \n",
    "            box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin: 20px 0;\">\n",
    "    <h2 style=\"color: white; margin: 0 0 15px 0;\">\n",
    "        ‚ö° Spark Web UI Ready for Jupyter!\n",
    "    </h2>\n",
    "    <div style=\"background: white; padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "        <a href=\"{spark_ui_url}\" target=\"_blank\" \n",
    "           style=\"color: #11998e; font-size: 18px; font-weight: bold; text-decoration: none;\">\n",
    "            üöÄ {spark_ui_url}\n",
    "        </a>\n",
    "    </div>\n",
    "    <p style=\"color: white; margin: 15px 0 5px 0; font-size: 14px;\">\n",
    "        Click the link above to open Spark Web UI in browser\n",
    "    </p>\n",
    "    <p style=\"color: rgba(255,255,255,0.9); margin: 5px 0 0 0; font-size: 12px;\">\n",
    "        ‚úÖ Jobs ‚Ä¢ ‚úÖ Stages ‚Ä¢ ‚úÖ DAG Visualizations ‚Ä¢ ‚úÖ SQL Queries\n",
    "    </p>\n",
    "</div>\n",
    "'''\n",
    "display(HTML(html))\n",
    "\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"   1. ‚úÖ Spark is running - click the green button above\")\n",
    "print(\"   2. üìÇ Upload your dataset (run Step 2)\")\n",
    "print(\"   3. ‚ö° Generate Spark jobs (run Step 3.5)\")\n",
    "print(\"   4. üîÑ Refresh the browser to see jobs in Spark UI!\")\n",
    "print(f\"\\nüìä Bookmark this URL: {spark_ui_url}\")\n",
    "print(\"\\n‚úÖ Ready for data analysis with Spark!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e33bea",
   "metadata": {},
   "source": [
    "## ‚úÖ Verify Java Installation (Windows)\n",
    "\n",
    "Run this cell first to check if Java is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ae883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Java installation\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"üîç Checking Java installation...\\n\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['java', '-version'], \n",
    "                          capture_output=True, \n",
    "                          text=True, \n",
    "                          shell=True)\n",
    "    \n",
    "    java_output = result.stderr if result.stderr else result.stdout\n",
    "    \n",
    "    if 'version' in java_output.lower():\n",
    "        print(\"‚úÖ Java is installed!\")\n",
    "        print(\"\\n\" + java_output)\n",
    "        print(\"\\nüéâ You're ready to use Spark!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Java check returned unexpected output\")\n",
    "        print(java_output)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Java is NOT installed!\")\n",
    "    print(\"\\nüì• Please install Java:\")\n",
    "    print(\"   1. Download from: https://www.oracle.com/java/technologies/downloads/\")\n",
    "    print(\"   2. Or install OpenJDK: https://adoptium.net/\")\n",
    "    print(\"   3. Recommended: Java 8 or Java 11\")\n",
    "    print(\"\\n   After installation, restart Jupyter and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking Java: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c0384",
   "metadata": {},
   "source": [
    "## üì§ Step 2: Upload Your Dataset\n",
    "\n",
    "**Supported formats:** CSV files\n",
    "\n",
    "**Expected columns (flexible):**\n",
    "- Product name/title\n",
    "- Price\n",
    "- Rating/Rate\n",
    "- Reviews/Review text\n",
    "- Category (optional)\n",
    "- Discount (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d7a1c",
   "metadata": {},
   "source": [
    "## üìÅ Quick Load (Alternative for Jupyter)\n",
    "\n",
    "If you prefer, directly specify the CSV path in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Load - Directly specify CSV path (easier for Jupyter)\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Change this to your CSV file path\n",
    "csv_path = \"BoatProduct.csv\"  # Or use full path like: r\"D:\\project zip flies\\scala project\\scala project\\BoatProduct.csv\"\n",
    "\n",
    "print(f\"üìÇ Current directory: {os.getcwd()}\")\n",
    "print(f\"üìÑ Loading: {csv_path}\\n\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"‚úÖ Successfully loaded!\")\n",
    "    print(f\"üìä Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"\\nüìã Columns: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(\"\\nüìë First 3 rows:\")\n",
    "    display(df.head(3))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File not found: {csv_path}\")\n",
    "    print(f\"\\nüí° Available CSV files in current directory:\")\n",
    "    csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "    for i, f in enumerate(csv_files, 1):\n",
    "        print(f\"   {i}. {f}\")\n",
    "    if csv_files:\n",
    "        print(f\"\\nüëâ Update csv_path to one of these files\")\n",
    "    else:\n",
    "        print(f\"   (No CSV files found)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0db9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload CSV file - Works in both Jupyter and Colab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set styling\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"üìÅ Please select your CSV file...\\n\")\n",
    "\n",
    "# Check if running in Colab or local Jupyter\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"üåê Google Colab detected - using file upload widget\")\n",
    "    uploaded = files.upload()\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    \n",
    "except ImportError:\n",
    "    # Running in local Jupyter\n",
    "    print(\"üíª Local Jupyter detected\")\n",
    "    print(\"\\nüìã Options:\")\n",
    "    print(\"   1. Enter full path to your CSV file\")\n",
    "    print(\"   2. Or place CSV in notebook directory and enter filename\")\n",
    "    print(f\"\\nüìÇ Current directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Show available CSV files in current directory\n",
    "    csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "    if csv_files:\n",
    "        print(f\"\\nüìÑ CSV files found in current directory:\")\n",
    "        for i, f in enumerate(csv_files, 1):\n",
    "            print(f\"   {i}. {f}\")\n",
    "    \n",
    "    filename = input(\"\\nüëâ Enter CSV filename or path: \").strip().strip('\"')\n",
    "    \n",
    "    # If user entered a number, use that file from the list\n",
    "    if filename.isdigit() and csv_files:\n",
    "        idx = int(filename) - 1\n",
    "        if 0 <= idx < len(csv_files):\n",
    "            filename = csv_files[idx]\n",
    "            print(f\"‚úÖ Selected: {filename}\")\n",
    "\n",
    "# Load the CSV file\n",
    "try:\n",
    "    df = pd.read_csv(filename)\n",
    "    print(f\"\\n‚úÖ Successfully loaded: {filename}\")\n",
    "    print(f\"üìä Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ùå File not found: {filename}\")\n",
    "    print(f\"\\nüí° Tips:\")\n",
    "    print(f\"   ‚Ä¢ Use full path: C:\\\\Users\\\\YourName\\\\Documents\\\\data.csv\")\n",
    "    print(f\"   ‚Ä¢ Or copy file to: {os.getcwd()}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading file: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66115d",
   "metadata": {},
   "source": [
    "## üîç Step 3: Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3754f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(filename, encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä DATASET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìà Total Records: {len(df):,}\")\n",
    "print(f\"üìã Total Columns: {len(df.columns)}\")\n",
    "print(f\"üíæ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\nüìë Column Names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"   {i}. {col} ({df[col].dtype})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç FIRST 5 ROWS\")\n",
    "print(\"=\"*70)\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä DATA TYPES & MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "info_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Data Type': df.dtypes.values,\n",
    "    'Non-Null Count': df.count().values,\n",
    "    'Null Count': df.isnull().sum().values,\n",
    "    'Null %': (df.isnull().sum() / len(df) * 100).values\n",
    "})\n",
    "display(info_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà STATISTICAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f19a4",
   "metadata": {},
   "source": [
    "## üßπ Step 4: Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c95d9",
   "metadata": {},
   "source": [
    "## ‚ö° Step 3.5: Process Data with Spark (Generate Jobs)\n",
    "\n",
    "This step converts your data to Spark DataFrames and runs analysis to generate jobs visible in Spark Web UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac89db30",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Python 3.13 Compatibility Issue\n",
    "\n",
    "**Current Status:** You have Python 3.13, which is incompatible with PySpark 3.5.0\n",
    "\n",
    "**Impact:** \n",
    "- ‚ùå Step 3.5 & 3.6 will crash\n",
    "- ‚ùå No Spark jobs will appear in Web UI\n",
    "- ‚ùå PySpark DataFrame operations fail\n",
    "\n",
    "**üéØ TO SEE SPARK JOBS - YOU MUST:**\n",
    "\n",
    "### Switch to Python 3.11 or 3.10\n",
    "\n",
    "**Fastest Method (if you have Anaconda):**\n",
    "\n",
    "Open PowerShell terminal and run:\n",
    "```powershell\n",
    "conda create -n spark-env python=3.11 -y\n",
    "conda activate spark-env\n",
    "pip install pyspark findspark pandas numpy matplotlib seaborn plotly textblob jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Then reopen this notebook in the new Jupyter session.\n",
    "\n",
    "**Verification:**\n",
    "After switching Python versions, run this in a cell:\n",
    "```python\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "# Should show 3.11.x\n",
    "```\n",
    "\n",
    "**Once on Python 3.11:**\n",
    "- ‚úÖ Run Step 3.5 - Spark jobs will work!\n",
    "- ‚úÖ Spark Web UI will show all jobs\n",
    "- ‚úÖ DAG visualizations visible\n",
    "- ‚úÖ Complete Spark functionality\n",
    "\n",
    "---\n",
    "\n",
    "**Alternative:** See Cell 1 for detailed downgrade instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Check if Ready for Spark Jobs\n",
    "\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç SPARK JOBS READINESS CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check 1: Python Version\n",
    "python_ver = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "print(f\"\\n1. Python Version: {python_ver}\")\n",
    "\n",
    "if sys.version_info >= (3, 13):\n",
    "    print(\"   ‚ùå Python 3.13 - NOT compatible with PySpark\")\n",
    "    print(\"   ‚ö†Ô∏è  Spark jobs WILL crash\")\n",
    "    print(\"   ‚úÖ Solution: Switch to Python 3.11\")\n",
    "    ready = False\n",
    "elif sys.version_info >= (3, 11):\n",
    "    print(\"   ‚úÖ Python 3.11/3.12 - Compatible!\")\n",
    "    ready = True\n",
    "elif sys.version_info >= (3, 8):\n",
    "    print(\"   ‚úÖ Python 3.8-3.10 - Compatible!\")\n",
    "    ready = True\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Old Python version - consider upgrading\")\n",
    "    ready = False\n",
    "\n",
    "# Check 2: Spark Session\n",
    "print(f\"\\n2. Spark Session:\")\n",
    "try:\n",
    "    if spark:\n",
    "        print(f\"   ‚úÖ Spark is running (v{spark.version})\")\n",
    "        print(f\"   üìä Web UI: {spark.sparkContext.uiWebUrl}\")\n",
    "except NameError:\n",
    "    print(\"   ‚ùå Spark not initialized\")\n",
    "    print(\"   ‚úÖ Solution: Run Cell 5 (Windows Spark Setup)\")\n",
    "    ready = False\n",
    "\n",
    "# Check 3: Data Loaded\n",
    "print(f\"\\n3. Dataset:\")\n",
    "try:\n",
    "    if df is not None:\n",
    "        print(f\"   ‚úÖ Data loaded: {len(df):,} rows\")\n",
    "except NameError:\n",
    "    print(\"   ‚ùå No data loaded\")\n",
    "    print(\"   ‚úÖ Solution: Run Cell 19 or 20 (Upload CSV)\")\n",
    "    ready = False\n",
    "\n",
    "# Final verdict\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if ready:\n",
    "    print(\"üéâ ALL SYSTEMS GO! Ready to run Spark jobs!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n‚úÖ Next step: Run Step 3.5 (Cell 26)\")\n",
    "    print(\"   Spark jobs will appear in Web UI!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  NOT READY - Fix issues above first\")\n",
    "    print(\"=\"*70)\n",
    "    if sys.version_info >= (3, 13):\n",
    "        print(\"\\nüîß CRITICAL: Python 3.13 detected!\")\n",
    "        print(\"\\n   Quick fix commands:\")\n",
    "        print(\"   conda create -n spark-env python=3.11 -y\")\n",
    "        print(\"   conda activate spark-env\")\n",
    "        print(\"   pip install pyspark findspark pandas numpy matplotlib seaborn plotly textblob jupyter\")\n",
    "        print(\"   jupyter notebook\")\n",
    "        print(\"\\n   Then reopen this notebook\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç PYTHON VERSION CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "python_version = f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\"\n",
    "print(f\"Current Python: {python_version}\")\n",
    "\n",
    "# Check if Python version is compatible\n",
    "if sys.version_info >= (3, 13):\n",
    "    print(\"\\n‚ö†Ô∏è  Python 3.13+ detected - NOT compatible with PySpark 3.5.0\")\n",
    "    print(\"‚ùå Spark jobs will crash\")\n",
    "    compatible = False\n",
    "elif sys.version_info >= (3, 12):\n",
    "    print(\"\\n‚ö†Ô∏è  Python 3.12 detected - May have worker crash issues\")\n",
    "    print(\"üí° Recommend: Use Python 3.11 or 3.10 for best results\")\n",
    "    compatible = \"maybe\"\n",
    "else:\n",
    "    print(\"\\n‚úÖ Python version compatible with PySpark\")\n",
    "    compatible = True\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Only attempt Spark jobs if compatible\n",
    "if compatible == False:\n",
    "    print(\"\\n‚ùå SKIPPING SPARK JOBS - Incompatible Python version\")\n",
    "    print(\"\\nüìä GOOD NEWS: You can still complete the full analysis!\")\n",
    "    print(\"   ‚úÖ Skip this cell\")\n",
    "    print(\"   ‚úÖ Run Cell 21 onwards for complete pandas analysis\")\n",
    "    print(\"   ‚úÖ Get all visualizations, statistics, and insights\")\n",
    "    print(\"\\nüí° To see Spark jobs in Web UI:\")\n",
    "    print(\"   ‚Ä¢ Install Python 3.11 (not 3.12 or 3.13)\")\n",
    "    print(\"   ‚Ä¢ Create new environment: py -3.11 -m venv spark-venv\")\n",
    "    print(\"   ‚Ä¢ Activate: .\\\\spark-venv\\\\Scripts\\\\activate\")\n",
    "    print(\"   ‚Ä¢ Install packages: pip install pyspark jupyter pandas matplotlib seaborn plotly textblob\")\n",
    "    print(\"   ‚Ä¢ Start: jupyter notebook\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "elif compatible == \"maybe\":\n",
    "    print(\"\\n‚ö†Ô∏è  ATTEMPTING SPARK JOBS (May fail with Python 3.12)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nüìä Converting pandas DataFrame to Spark RDD...\")\n",
    "        print(\"   Using RDD operations (better compatibility)...\\n\")\n",
    "        \n",
    "        # Convert DataFrame to list of rows\n",
    "        data_rows = df.values.tolist()\n",
    "        column_names = df.columns.tolist()\n",
    "        \n",
    "        # Create RDD from the data\n",
    "        rdd = spark.sparkContext.parallelize(data_rows)\n",
    "        \n",
    "        print(f\"‚úÖ Created Spark RDD successfully\")\n",
    "        print(f\"   Partitions: {rdd.getNumPartitions()}\")\n",
    "        \n",
    "        # Run simplified RDD operations\n",
    "        print(\"\\nüîÑ Running Spark RDD operations...\")\n",
    "        \n",
    "        # Job 1: Count operation\n",
    "        print(\"\\n1Ô∏è‚É£ RDD Count...\")\n",
    "        count = rdd.count()\n",
    "        print(f\"   ‚úÖ Total rows: {count:,}\")\n",
    "        \n",
    "        # Job 2: Take sample\n",
    "        print(\"\\n2Ô∏è‚É£ Taking sample...\")\n",
    "        sample = rdd.take(3)\n",
    "        print(f\"   ‚úÖ Sample retrieved: {len(sample)} rows\")\n",
    "        \n",
    "        # Job 3: Simple map operation\n",
    "        print(\"\\n3Ô∏è‚É£ Map operation...\")\n",
    "        mapped = rdd.map(lambda x: 1).take(10)\n",
    "        print(f\"   ‚úÖ Map completed\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ Spark RDD processing complete!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nüìä CHECK SPARK WEB UI: {spark_ui_url}\")\n",
    "        print(\"   You should see completed jobs in the Jobs tab\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"\\n‚ùå Spark job failed: {type(e).__name__}\")\n",
    "        \n",
    "        if \"Python worker\" in error_msg or \"crashed\" in error_msg:\n",
    "            print(\"\\nüîß PYTHON WORKER CRASHED\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"\\nPython 3.12.4 has compatibility issues with PySpark.\")\n",
    "            print(\"\\n‚úÖ SOLUTION: Skip this cell and use pandas analysis\")\n",
    "            print(\"   ‚Ä¢ Cell 21 onwards: Complete data analysis with pandas\")\n",
    "            print(\"   ‚Ä¢ Get all visualizations, statistics, and insights\")\n",
    "            print(\"   ‚Ä¢ No Spark required!\")\n",
    "            print(\"\\nüí° To fix Spark jobs:\")\n",
    "            print(\"   ‚Ä¢ Install Python 3.11 (download from python.org)\")\n",
    "            print(\"   ‚Ä¢ Create new environment with Python 3.11\")\n",
    "            print(\"   ‚Ä¢ Reinstall packages and rerun notebook\")\n",
    "            print(\"=\"*70)\n",
    "        else:\n",
    "            print(f\"   Error: {error_msg[:200]}\")\n",
    "\n",
    "else:\n",
    "    # Python 3.11 or lower - should work\n",
    "    print(\"\\n‚ö° SPARK RDD PROCESSING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nüìä Converting pandas DataFrame to Spark RDD...\")\n",
    "        \n",
    "        # Convert DataFrame to list of rows\n",
    "        data_rows = df.values.tolist()\n",
    "        column_names = df.columns.tolist()\n",
    "        \n",
    "        # Create RDD from the data\n",
    "        rdd = spark.sparkContext.parallelize(data_rows)\n",
    "        \n",
    "        print(f\"‚úÖ Created Spark RDD successfully\")\n",
    "        print(f\"   Partitions: {rdd.getNumPartitions()}\")\n",
    "        \n",
    "        # Run RDD operations to generate jobs\n",
    "        print(\"\\nüîÑ Running Spark RDD operations (check Web UI for jobs)...\")\n",
    "        \n",
    "        # Job 1: Count\n",
    "        print(\"\\n1Ô∏è‚É£ RDD Count...\")\n",
    "        count = rdd.count()\n",
    "        print(f\"   Total rows: {count:,}\")\n",
    "        \n",
    "        # Job 2: Sample\n",
    "        print(\"\\n2Ô∏è‚É£ Taking sample...\")\n",
    "        sample = rdd.take(5)\n",
    "        print(f\"   Sample: {len(sample)} rows\")\n",
    "        \n",
    "        # Job 3: Map\n",
    "        print(\"\\n3Ô∏è‚É£ Map operation...\")\n",
    "        first_col = rdd.map(lambda row: row[0] if len(row) > 0 else None).take(5)\n",
    "        print(f\"   Mapped: {first_col}\")\n",
    "        \n",
    "        # Job 4: Filter\n",
    "        print(\"\\n4Ô∏è‚É£ Filter operation...\")\n",
    "        filtered = rdd.filter(lambda row: row is not None).count()\n",
    "        print(f\"   Filtered rows: {filtered:,}\")\n",
    "        \n",
    "        # Job 5: Reduce\n",
    "        print(\"\\n5Ô∏è‚É£ Reduce operation...\")\n",
    "        total = rdd.map(lambda x: 1).reduce(lambda a, b: a + b)\n",
    "        print(f\"   Total count: {total:,}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ Spark RDD processing complete!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nüìä CHECK SPARK WEB UI: {spark_ui_url}\")\n",
    "        print(\"\\n   You should see:\")\n",
    "        print(\"   ‚úÖ 5+ completed jobs\")\n",
    "        print(\"   ‚úÖ RDD operations in Jobs tab\")\n",
    "        print(\"   ‚úÖ DAG visualizations\")\n",
    "        print(\"   ‚úÖ Task details in Stages tab\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "    except NameError as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  NameError: {e}\")\n",
    "        print(\"\\nüìã Make sure you've run:\")\n",
    "        print(\"   1. Cell 7 - Windows Spark Setup\")\n",
    "        print(\"   2. Cell 12 or 13 - Load CSV data\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"\\n‚ùå Error: {type(e).__name__}\")\n",
    "        print(f\"   {error_msg[:200]}\")\n",
    "        \n",
    "        if \"Python worker\" in error_msg or \"crashed\" in error_msg:\n",
    "            print(\"\\nüîß Unexpected worker crash\")\n",
    "            print(\"\\nüí° Alternative: Skip this cell and run Cell 21+ for pandas analysis\")\n",
    "        \n",
    "        import traceback\n",
    "        print(\"\\nüìã Full traceback:\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd96d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a copy for processing\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(\"üßπ Starting data cleaning...\\n\")\n",
    "\n",
    "# Detect column names (flexible mapping)\n",
    "column_mapping = {}\n",
    "\n",
    "for col in df_clean.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'product' in col_lower and 'name' in col_lower:\n",
    "        column_mapping['product_name'] = col\n",
    "    elif 'price' in col_lower:\n",
    "        column_mapping['price'] = col\n",
    "    elif 'rate' in col_lower or 'rating' in col_lower:\n",
    "        column_mapping['rating'] = col\n",
    "    elif 'review' in col_lower and 'number' not in col_lower and 'count' not in col_lower:\n",
    "        column_mapping['review_text'] = col\n",
    "    elif 'review' in col_lower and ('number' in col_lower or 'count' in col_lower):\n",
    "        column_mapping['review_count'] = col\n",
    "    elif 'discount' in col_lower:\n",
    "        column_mapping['discount'] = col\n",
    "    elif 'summary' in col_lower:\n",
    "        column_mapping['summary'] = col\n",
    "\n",
    "print(\"üìã Detected columns:\")\n",
    "for key, val in column_mapping.items():\n",
    "    print(f\"   {key}: {val}\")\n",
    "\n",
    "# Clean price column\n",
    "if 'price' in column_mapping:\n",
    "    def clean_price(price_str):\n",
    "        if pd.isna(price_str):\n",
    "            return np.nan\n",
    "        price_str = str(price_str)\n",
    "        # Remove currency symbols, commas, and text\n",
    "        price_str = re.sub(r'[^0-9.]', '', price_str)\n",
    "        try:\n",
    "            return float(price_str)\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    df_clean['price_cleaned'] = df_clean[column_mapping['price']].apply(clean_price)\n",
    "    print(\"\\n‚úÖ Price column cleaned\")\n",
    "\n",
    "# Clean rating column\n",
    "if 'rating' in column_mapping:\n",
    "    def clean_rating(rating_str):\n",
    "        if pd.isna(rating_str):\n",
    "            return np.nan\n",
    "        rating_str = str(rating_str)\n",
    "        # Extract numeric rating\n",
    "        match = re.search(r'(\\d+\\.?\\d*)', rating_str)\n",
    "        if match:\n",
    "            rating = float(match.group(1))\n",
    "            # Normalize to 5-point scale if needed\n",
    "            if rating > 5:\n",
    "                rating = rating / 10 * 5\n",
    "            return rating\n",
    "        return np.nan\n",
    "    \n",
    "    df_clean['rating_cleaned'] = df_clean[column_mapping['rating']].apply(clean_rating)\n",
    "    print(\"‚úÖ Rating column cleaned\")\n",
    "\n",
    "# Clean review count\n",
    "if 'review_count' in column_mapping:\n",
    "    def clean_review_count(count_str):\n",
    "        if pd.isna(count_str):\n",
    "            return 0\n",
    "        count_str = str(count_str)\n",
    "        # Extract numbers\n",
    "        numbers = re.findall(r'\\d+', count_str)\n",
    "        if numbers:\n",
    "            return int(numbers[0])\n",
    "        return 0\n",
    "    \n",
    "    df_clean['review_count_cleaned'] = df_clean[column_mapping['review_count']].apply(clean_review_count)\n",
    "    print(\"‚úÖ Review count cleaned\")\n",
    "\n",
    "# Clean discount\n",
    "if 'discount' in column_mapping:\n",
    "    def clean_discount(discount_str):\n",
    "        if pd.isna(discount_str):\n",
    "            return 0\n",
    "        discount_str = str(discount_str)\n",
    "        numbers = re.findall(r'\\d+', discount_str)\n",
    "        if numbers:\n",
    "            return int(numbers[0])\n",
    "        return 0\n",
    "    \n",
    "    df_clean['discount_cleaned'] = df_clean[column_mapping['discount']].apply(clean_discount)\n",
    "    print(\"‚úÖ Discount column cleaned\")\n",
    "\n",
    "# Sentiment analysis on reviews\n",
    "if 'review_text' in column_mapping or 'summary' in column_mapping:\n",
    "    review_col = column_mapping.get('review_text') or column_mapping.get('summary')\n",
    "    \n",
    "    def get_sentiment(text):\n",
    "        if pd.isna(text) or str(text).strip() == '':\n",
    "            return 'Neutral', 0.0\n",
    "        try:\n",
    "            analysis = TextBlob(str(text))\n",
    "            polarity = analysis.sentiment.polarity\n",
    "            if polarity > 0.1:\n",
    "                return 'Positive', polarity\n",
    "            elif polarity < -0.1:\n",
    "                return 'Negative', polarity\n",
    "            else:\n",
    "                return 'Neutral', polarity\n",
    "        except:\n",
    "            return 'Neutral', 0.0\n",
    "    \n",
    "    print(\"\\nü§ñ Performing sentiment analysis...\")\n",
    "    df_clean[['sentiment', 'sentiment_score']] = df_clean[review_col].apply(\n",
    "        lambda x: pd.Series(get_sentiment(x))\n",
    "    )\n",
    "    print(\"‚úÖ Sentiment analysis completed\")\n",
    "\n",
    "# Remove duplicates\n",
    "before_dup = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "after_dup = len(df_clean)\n",
    "print(f\"\\nüóëÔ∏è  Removed {before_dup - after_dup} duplicate rows\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data cleaning completed!\")\n",
    "print(f\"üìä Final dataset size: {len(df_clean):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612da2d",
   "metadata": {},
   "source": [
    "## üìä Step 5: Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbdf32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìà STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Price statistics\n",
    "if 'price_cleaned' in df_clean.columns:\n",
    "    print(\"\\nüí∞ PRICE STATISTICS:\")\n",
    "    price_stats = df_clean['price_cleaned'].describe()\n",
    "    print(f\"   Mean Price: ‚Çπ{price_stats['mean']:,.2f}\")\n",
    "    print(f\"   Median Price: ‚Çπ{price_stats['50%']:,.2f}\")\n",
    "    print(f\"   Min Price: ‚Çπ{price_stats['min']:,.2f}\")\n",
    "    print(f\"   Max Price: ‚Çπ{price_stats['max']:,.2f}\")\n",
    "    print(f\"   Std Dev: ‚Çπ{price_stats['std']:,.2f}\")\n",
    "\n",
    "# Rating statistics\n",
    "if 'rating_cleaned' in df_clean.columns:\n",
    "    print(\"\\n‚≠ê RATING STATISTICS:\")\n",
    "    rating_stats = df_clean['rating_cleaned'].describe()\n",
    "    print(f\"   Mean Rating: {rating_stats['mean']:.2f}/5.0\")\n",
    "    print(f\"   Median Rating: {rating_stats['50%']:.2f}/5.0\")\n",
    "    print(f\"   Min Rating: {rating_stats['min']:.2f}\")\n",
    "    print(f\"   Max Rating: {rating_stats['max']:.2f}\")\n",
    "    print(f\"   Std Dev: {rating_stats['std']:.2f}\")\n",
    "    \n",
    "    # Rating distribution\n",
    "    print(\"\\n   Rating Distribution:\")\n",
    "    rating_dist = df_clean['rating_cleaned'].value_counts().sort_index(ascending=False)\n",
    "    for rating, count in rating_dist.head(5).items():\n",
    "        percentage = (count / len(df_clean)) * 100\n",
    "        print(f\"   {rating:.1f} ‚≠ê: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Sentiment statistics\n",
    "if 'sentiment' in df_clean.columns:\n",
    "    print(\"\\nüòä SENTIMENT DISTRIBUTION:\")\n",
    "    sentiment_counts = df_clean['sentiment'].value_counts()\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        percentage = (count / len(df_clean)) * 100\n",
    "        emoji = 'üòä' if sentiment == 'Positive' else 'üòê' if sentiment == 'Neutral' else 'üòû'\n",
    "        print(f\"   {emoji} {sentiment}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Review count statistics\n",
    "if 'review_count_cleaned' in df_clean.columns:\n",
    "    print(\"\\nüí¨ REVIEW STATISTICS:\")\n",
    "    review_stats = df_clean['review_count_cleaned'].describe()\n",
    "    print(f\"   Total Reviews: {df_clean['review_count_cleaned'].sum():,}\")\n",
    "    print(f\"   Avg Reviews per Product: {review_stats['mean']:.2f}\")\n",
    "    print(f\"   Max Reviews: {review_stats['max']:.0f}\")\n",
    "\n",
    "# Product statistics\n",
    "if 'product_name' in column_mapping:\n",
    "    print(\"\\nüì¶ PRODUCT STATISTICS:\")\n",
    "    unique_products = df_clean[column_mapping['product_name']].nunique()\n",
    "    print(f\"   Unique Products: {unique_products:,}\")\n",
    "    print(f\"   Total Records: {len(df_clean):,}\")\n",
    "    print(f\"   Avg Reviews per Product: {len(df_clean) / unique_products:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2312c339",
   "metadata": {},
   "source": [
    "## üìä Step 6: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0dca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure df_clean is a pandas DataFrame (convert from Spark if needed)\n",
    "if hasattr(df_clean, 'toPandas'):\n",
    "    print(\"üîÑ Converting Spark DataFrame to pandas...\")\n",
    "    df_clean = df_clean.toPandas()\n",
    "    print(\"‚úÖ Conversion complete\")\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Price Distribution', 'Rating Distribution', \n",
    "                    'Sentiment Analysis', 'Top 10 Products by Reviews'),\n",
    "    specs=[[{'type': 'histogram'}, {'type': 'histogram'}],\n",
    "           [{'type': 'pie'}, {'type': 'bar'}]]\n",
    ")\n",
    "\n",
    "# 1. Price Distribution\n",
    "if 'price_cleaned' in df_clean.columns:\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df_clean['price_cleaned'], name='Price', \n",
    "                     marker_color='#3498db', nbinsx=50),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# 2. Rating Distribution\n",
    "if 'rating_cleaned' in df_clean.columns:\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df_clean['rating_cleaned'], name='Rating',\n",
    "                     marker_color='#f39c12', nbinsx=20),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# 3. Sentiment Pie Chart\n",
    "if 'sentiment' in df_clean.columns:\n",
    "    sentiment_counts = df_clean['sentiment'].value_counts()\n",
    "    colors = {'Positive': '#2ecc71', 'Neutral': '#95a5a6', 'Negative': '#e74c3c'}\n",
    "    pie_colors = [colors.get(s, '#3498db') for s in sentiment_counts.index]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=sentiment_counts.index, values=sentiment_counts.values,\n",
    "               marker=dict(colors=pie_colors)),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 4. Top Products by Review Count\n",
    "if 'product_name' in column_mapping and 'review_count_cleaned' in df_clean.columns:\n",
    "    top_products = df_clean.groupby(column_mapping['product_name'])['review_count_cleaned'].sum()\n",
    "    top_products = top_products.nlargest(10).sort_values()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(y=top_products.index, x=top_products.values,\n",
    "               orientation='h', marker_color='#9b59b6'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    showlegend=False,\n",
    "    title_text=\"üìä Comprehensive Dataset Analysis Dashboard\",\n",
    "    title_font_size=20\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"‚úÖ Dashboard generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdebc587",
   "metadata": {},
   "source": [
    "## üìà Step 7: Advanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure df_clean is a pandas DataFrame (convert from Spark if needed)\n",
    "try:\n",
    "    # Check if it's a Spark DataFrame by trying to access toPandas method\n",
    "    if hasattr(df_clean, 'toPandas'):\n",
    "        print(\"üîÑ Converting Spark DataFrame to pandas for visualizations...\")\n",
    "        df_clean = df_clean.toPandas()\n",
    "        print(\"‚úÖ Conversion complete\\n\")\n",
    "    # Also check by type name\n",
    "    elif type(df_clean).__name__ == 'DataFrame' and 'pyspark' in str(type(df_clean).__module__):\n",
    "        print(\"üîÑ Converting Spark DataFrame to pandas for visualizations...\")\n",
    "        df_clean = df_clean.toPandas()\n",
    "        print(\"‚úÖ Conversion complete\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error during conversion check: {e}\")\n",
    "\n",
    "# Verify it's pandas now\n",
    "print(f\"üìä DataFrame type: {type(df_clean)}\")\n",
    "print(f\"   Shape: {df_clean.shape if hasattr(df_clean, 'shape') else 'N/A'}\")\n",
    "\n",
    "# Price vs Rating Scatter Plot\n",
    "if 'price_cleaned' in df_clean.columns and 'rating_cleaned' in df_clean.columns:\n",
    "    # Sample data for better performance (using pandas methods)\n",
    "    total_rows = df_clean.shape[0]\n",
    "    sample_size = 1000 if total_rows > 1000 else total_rows\n",
    "    df_sample = df_clean.sample(n=sample_size) if total_rows > 0 else df_clean\n",
    "    \n",
    "    fig = px.scatter(\n",
    "        df_sample,\n",
    "        x='price_cleaned',\n",
    "        y='rating_cleaned',\n",
    "        color='sentiment' if 'sentiment' in df_clean.columns else None,\n",
    "        size='review_count_cleaned' if 'review_count_cleaned' in df_clean.columns else None,\n",
    "        title='üí∞ Price vs Rating Analysis',\n",
    "        labels={'price_cleaned': 'Price (‚Çπ)', 'rating_cleaned': 'Rating'},\n",
    "        color_discrete_map={'Positive': '#2ecc71', 'Neutral': '#95a5a6', 'Negative': '#e74c3c'},\n",
    "        height=500\n",
    "    )\n",
    "    fig.update_layout(xaxis_title='Price (‚Çπ)', yaxis_title='Rating')\n",
    "    fig.show()\n",
    "\n",
    "# Box plot for rating distribution by sentiment\n",
    "if 'rating_cleaned' in df_clean.columns and 'sentiment' in df_clean.columns:\n",
    "    fig = px.box(\n",
    "        df_clean,\n",
    "        x='sentiment',\n",
    "        y='rating_cleaned',\n",
    "        color='sentiment',\n",
    "        title='‚≠ê Rating Distribution by Sentiment',\n",
    "        labels={'rating_cleaned': 'Rating', 'sentiment': 'Sentiment'},\n",
    "        color_discrete_map={'Positive': '#2ecc71', 'Neutral': '#95a5a6', 'Negative': '#e74c3c'},\n",
    "        height=500\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Price categories\n",
    "if 'price_cleaned' in df_clean.columns:\n",
    "    def categorize_price(price):\n",
    "        if pd.isna(price):\n",
    "            return 'Unknown'\n",
    "        if price < 1000:\n",
    "            return 'Budget (< ‚Çπ1K)'\n",
    "        elif price < 5000:\n",
    "            return 'Economy (‚Çπ1K-5K)'\n",
    "        elif price < 15000:\n",
    "            return 'Mid-Range (‚Çπ5K-15K)'\n",
    "        elif price < 50000:\n",
    "            return 'Premium (‚Çπ15K-50K)'\n",
    "        else:\n",
    "            return 'Luxury (> ‚Çπ50K)'\n",
    "    \n",
    "    df_clean['price_category'] = df_clean['price_cleaned'].apply(categorize_price)\n",
    "    \n",
    "    price_cat_counts = df_clean['price_category'].value_counts()\n",
    "    order = ['Budget (< ‚Çπ1K)', 'Economy (‚Çπ1K-5K)', 'Mid-Range (‚Çπ5K-15K)', \n",
    "             'Premium (‚Çπ15K-50K)', 'Luxury (> ‚Çπ50K)']\n",
    "    price_cat_counts = price_cat_counts.reindex([o for o in order if o in price_cat_counts.index])\n",
    "    \n",
    "    fig = px.bar(\n",
    "        x=price_cat_counts.index,\n",
    "        y=price_cat_counts.values,\n",
    "        title='üíµ Product Distribution by Price Category',\n",
    "        labels={'x': 'Price Category', 'y': 'Number of Products'},\n",
    "        color=price_cat_counts.values,\n",
    "        color_continuous_scale='Viridis',\n",
    "        height=500\n",
    "    )\n",
    "    fig.update_layout(xaxis_title='Price Category', yaxis_title='Number of Products')\n",
    "    fig.show()\n",
    "\n",
    "print(\"‚úÖ Advanced visualizations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca32f96",
   "metadata": {},
   "source": [
    "## üéØ Step 8: Key Insights & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3dae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure df_clean is pandas DataFrame\n",
    "if hasattr(df_clean, 'toPandas'):\n",
    "    print(\"üîÑ Converting Spark DataFrame to pandas...\")\n",
    "    df_clean = df_clean.toPandas()\n",
    "    print(\"‚úÖ Conversion complete\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ KEY INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Price insights\n",
    "if 'price_cleaned' in df_clean.columns:\n",
    "    avg_price = df_clean['price_cleaned'].mean()\n",
    "    median_price = df_clean['price_cleaned'].median()\n",
    "    \n",
    "    if avg_price > median_price * 1.5:\n",
    "        insights.append(f\"üìä The average price (‚Çπ{avg_price:,.2f}) is significantly higher than the median (‚Çπ{median_price:,.2f}), indicating the presence of high-priced premium products skewing the distribution.\")\n",
    "    \n",
    "    price_range = df_clean['price_cleaned'].max() - df_clean['price_cleaned'].min()\n",
    "    insights.append(f\"üí∞ Price range spans ‚Çπ{price_range:,.2f}, from ‚Çπ{df_clean['price_cleaned'].min():,.2f} to ‚Çπ{df_clean['price_cleaned'].max():,.2f}.\")\n",
    "\n",
    "# Rating insights\n",
    "if 'rating_cleaned' in df_clean.columns:\n",
    "    avg_rating = df_clean['rating_cleaned'].mean()\n",
    "    high_rated = (df_clean['rating_cleaned'] >= 4.0).sum()\n",
    "    high_rated_pct = (high_rated / len(df_clean)) * 100\n",
    "    \n",
    "    insights.append(f\"‚≠ê Average rating is {avg_rating:.2f}/5.0. {high_rated_pct:.1f}% of products have ratings ‚â• 4.0, indicating generally positive customer satisfaction.\")\n",
    "    \n",
    "    if avg_rating < 3.5:\n",
    "        insights.append(\"‚ö†Ô∏è Overall ratings are below 3.5, suggesting room for quality improvement.\")\n",
    "    elif avg_rating >= 4.5:\n",
    "        insights.append(\"‚ú® Excellent average rating above 4.5, indicating high customer satisfaction!\")\n",
    "\n",
    "# Sentiment insights\n",
    "if 'sentiment' in df_clean.columns:\n",
    "    sentiment_dist = df_clean['sentiment'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    if 'Positive' in sentiment_dist.index:\n",
    "        pos_pct = sentiment_dist['Positive']\n",
    "        if pos_pct > 70:\n",
    "            insights.append(f\"üòä Strong positive sentiment ({pos_pct:.1f}%) in customer reviews - excellent product reception!\")\n",
    "        elif pos_pct < 40:\n",
    "            insights.append(f\"üòü Low positive sentiment ({pos_pct:.1f}%) - consider investigating customer concerns.\")\n",
    "    \n",
    "    if 'Negative' in sentiment_dist.index:\n",
    "        neg_pct = sentiment_dist['Negative']\n",
    "        if neg_pct > 20:\n",
    "            insights.append(f\"‚ö†Ô∏è Significant negative sentiment ({neg_pct:.1f}%) detected - review quality issues.\")\n",
    "\n",
    "# Price-Rating correlation\n",
    "if 'price_cleaned' in df_clean.columns and 'rating_cleaned' in df_clean.columns:\n",
    "    # Calculate correlation using pandas\n",
    "    correlation = df_clean[['price_cleaned', 'rating_cleaned']].corr().iloc[0, 1]\n",
    "    \n",
    "    # Use Python's built-in abs() instead of letting it be interpreted by PySpark\n",
    "    import builtins\n",
    "    if builtins.abs(correlation) > 0.5:\n",
    "        direction = \"positive\" if correlation > 0 else \"negative\"\n",
    "        insights.append(f\"üìà Strong {direction} correlation ({correlation:.2f}) between price and rating - {'higher' if correlation > 0 else 'lower'} priced products tend to have {'better' if correlation > 0 else 'worse'} ratings.\")\n",
    "    elif builtins.abs(correlation) < 0.2:\n",
    "        insights.append(f\"üí° Weak correlation ({correlation:.2f}) between price and rating - price doesn't strongly predict quality.\")\n",
    "\n",
    "# Review insights\n",
    "if 'review_count_cleaned' in df_clean.columns:\n",
    "    total_reviews = df_clean['review_count_cleaned'].sum()\n",
    "    insights.append(f\"üí¨ Total of {total_reviews:,} customer reviews analyzed, providing rich feedback data.\")\n",
    "    \n",
    "    no_reviews = (df_clean['review_count_cleaned'] == 0).sum()\n",
    "    if no_reviews > 0:\n",
    "        no_reviews_pct = (no_reviews / len(df_clean)) * 100\n",
    "        insights.append(f\"üìù {no_reviews_pct:.1f}% of products have no reviews - opportunity for review collection campaigns.\")\n",
    "\n",
    "# Discount insights\n",
    "if 'discount_cleaned' in df_clean.columns:\n",
    "    avg_discount = df_clean['discount_cleaned'].mean()\n",
    "    max_discount = df_clean['discount_cleaned'].max()\n",
    "    \n",
    "    if avg_discount > 20:\n",
    "        insights.append(f\"üè∑Ô∏è Average discount of {avg_discount:.1f}% indicates aggressive pricing strategy. Maximum discount: {max_discount:.0f}%.\")\n",
    "    \n",
    "    if 'rating_cleaned' in df_clean.columns:\n",
    "        high_discount_products = df_clean[df_clean['discount_cleaned'] > 30]\n",
    "        if len(high_discount_products) > 0:\n",
    "            avg_rating_discounted = high_discount_products['rating_cleaned'].mean()\n",
    "            avg_rating_normal = df_clean[df_clean['discount_cleaned'] <= 30]['rating_cleaned'].mean()\n",
    "            \n",
    "            if avg_rating_discounted < avg_rating_normal - 0.5:\n",
    "                insights.append(\"‚ö†Ô∏è Heavily discounted products have significantly lower ratings - possible quality concerns.\")\n",
    "\n",
    "# Display insights\n",
    "print()\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìã RECOMMENDATIONS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if 'rating_cleaned' in df_clean.columns and df_clean['rating_cleaned'].mean() < 3.5:\n",
    "    recommendations.append(\"üîß Focus on quality improvement initiatives\")\n",
    "    recommendations.append(\"üìû Implement customer feedback collection system\")\n",
    "\n",
    "if 'sentiment' in df_clean.columns:\n",
    "    neg_pct = (df_clean['sentiment'] == 'Negative').sum() / len(df_clean) * 100\n",
    "    if neg_pct > 20:\n",
    "        recommendations.append(\"üõ†Ô∏è Address common complaints mentioned in negative reviews\")\n",
    "        recommendations.append(\"üì¢ Improve customer communication and support\")\n",
    "\n",
    "if 'price_cleaned' in df_clean.columns:\n",
    "    if df_clean['price_cleaned'].std() / df_clean['price_cleaned'].mean() > 1:\n",
    "        recommendations.append(\"üí∞ Consider segmenting products into clear price tiers\")\n",
    "\n",
    "if 'review_count_cleaned' in df_clean.columns:\n",
    "    no_reviews_pct = (df_clean['review_count_cleaned'] == 0).sum() / len(df_clean) * 100\n",
    "    if no_reviews_pct > 30:\n",
    "        recommendations.append(\"üìù Launch review incentive programs for products with low engagement\")\n",
    "\n",
    "recommendations.append(\"üìä Continuously monitor these metrics to track improvement\")\n",
    "recommendations.append(\"üéØ Use insights for targeted marketing and product development\")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ed6d9",
   "metadata": {},
   "source": [
    "## üíæ Step 9: Export Processed Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned dataset\n",
    "output_filename = 'processed_dataset.csv'\n",
    "df_clean.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"‚úÖ Processed dataset saved as '{output_filename}'\")\n",
    "print(f\"üìä Total records: {len(df_clean):,}\")\n",
    "print(f\"üìã Total columns: {len(df_clean.columns)}\")\n",
    "\n",
    "# Show the full path where file was saved\n",
    "import os\n",
    "full_path = os.path.abspath(output_filename)\n",
    "print(f\"\\nüíæ File saved to: {full_path}\")\n",
    "print(f\"\\nüìÇ You can find it in the same directory as this notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c6f44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Summary\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "‚úÖ **Automated data loading** from CSV upload\n",
    "\n",
    "‚úÖ **Intelligent column detection** and cleaning\n",
    "\n",
    "‚úÖ **Comprehensive statistical analysis**\n",
    "\n",
    "‚úÖ **Interactive visualizations** with Plotly\n",
    "\n",
    "‚úÖ **AI-powered sentiment analysis** on reviews\n",
    "\n",
    "‚úÖ **Actionable insights** and recommendations\n",
    "\n",
    "‚úÖ **Processed data export** for further use\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. Upload your own CSV dataset\n",
    "2. Run all cells sequentially\n",
    "3. Review the insights and visualizations\n",
    "4. Download the processed data\n",
    "5. Use insights for business decisions\n",
    "\n",
    "---\n",
    "\n",
    "**Created for:** Dataset Analysis Prototype\n",
    "\n",
    "**Version:** 1.0\n",
    "\n",
    "**Platform:** Google Colab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
